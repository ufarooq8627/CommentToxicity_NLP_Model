# ğŸ›¡ï¸ Comment Toxicity Detection with Deep Learning

A real-time comment toxicity detection system powered by a **Bidirectional LSTM** deep learning model, deployed as an interactive **Streamlit** web application.

## ğŸ“‹ Project Overview

Online communities face challenges from toxic comments including harassment, hate speech, and offensive language. This project builds an automated system that analyzes text input and predicts toxicity across 6 categories:

| Label | Description |
|-------|-------------|
| **Toxic** | Generally toxic or rude |
| **Severe Toxic** | Extremely toxic content |
| **Obscene** | Obscene language |
| **Threat** | Threatening language |
| **Insult** | Insulting language |
| **Identity Hate** | Hate based on identity |

## ğŸ—ï¸ Architecture

```
Input Text â†’ Clean Text â†’ Tokenize â†’ Pad Sequences
    â†’ Embedding (128d)
    â†’ SpatialDropout1D (0.3)
    â†’ Bidirectional LSTM (64 units)
    â†’ GlobalMaxPooling1D
    â†’ Dense (64, ReLU) â†’ Dropout (0.3)
    â†’ Dense (6, Sigmoid) â†’ Multi-label Predictions
```

## ğŸš€ Setup & Installation

### 1. Clone / Download the project

```bash
cd CommentToxicity
```

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Download the dataset

Download `train.csv` and `test.csv` from the [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/data) and place them in the project root.

### 4. Train the model

```bash
python train_model.py
```

This will:
- Load and preprocess the data
- Train a BiLSTM model (~10 epochs with early stopping)
- Save model artifacts to the `model/` directory

### 5. Run the Streamlit app

```bash
streamlit run app.py
```

The app will open at `http://localhost:8501`.

## ğŸ“ Project Structure

```
CommentToxicity/
â”œâ”€â”€ train.csv                 # Training dataset
â”œâ”€â”€ test.csv                  # Test dataset
â”œâ”€â”€ train_model.py            # Model training script
â”œâ”€â”€ app.py                    # Streamlit web application
â”œâ”€â”€ comment_toxicity.ipynb    # Jupyter notebook (EDA & experiments)
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file
â””â”€â”€ model/                    # Generated after training
    â”œâ”€â”€ toxicity_model.keras  # Trained Keras model
    â”œâ”€â”€ tokenizer.pickle      # Fitted tokenizer
    â””â”€â”€ metrics.json          # Evaluation metrics
```

## ğŸŒ Streamlit App Features

1. **ğŸ” Single Prediction** â€” Enter any comment and get real-time toxicity scores with visual breakdown
2. **ğŸ“ Bulk Prediction** â€” Upload a CSV file with comments, get predictions for all rows, and download results
3. **ğŸ“Š Dashboard** â€” View data insights (class distribution, correlation heatmap), model performance (AUC-ROC, classification report), and training history charts

## ğŸ“Š Model Performance

The model is evaluated using:
- **Classification Report** (Precision, Recall, F1-Score per label)
- **AUC-ROC Score** (per label and macro average)

Results are saved in `model/metrics.json` and displayed in the Streamlit dashboard.

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**
- **TensorFlow / Keras** â€” Deep learning model
- **Streamlit** â€” Web application
- **scikit-learn** â€” Evaluation metrics
- **imbalanced-learn** â€” SMOTE for class balancing
- **pandas, numpy** â€” Data processing
- **matplotlib, seaborn** â€” Visualizations

## ğŸ“ License

This project is for educational purposes as part of the GUVI Deep Learning curriculum.
